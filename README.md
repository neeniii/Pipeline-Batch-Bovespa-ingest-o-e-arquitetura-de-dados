ğŸ“Œ VisÃ£o Geral
# Este projeto implementa um pipeline de ingestÃ£o e transformaÃ§Ã£o de dados da B3 utilizando os serviÃ§os da AWS.
# O objetivo Ã© coleta de dados, armazenamento, processamento, catalogaÃ§Ã£o e consulta no Athena.

ğŸ›  Arquitetura
- Scraping da B3 â†’ Captura os dados de aÃ§Ãµes e salva no Amazon S3 em formato Parquet, particionado por data.**(local no vs code)**

- Lambda â†’ Dispara automaticamente quando um novo arquivo Ã© adicionado ao bucket e inicia o AWS Glue Job.**(O script foi feito direto no AWS no lambda)**

- AWS Glue (Visual) â†’ Processa os dados, aplica transformaÃ§Ãµes e salva no S3 na pasta refined/, tambÃ©m em formato Parquet, particionado por data e nome da aÃ§Ã£o.

- Glue Crawler â†’ Atualiza o Glue Catalog para disponibilizar os dados no Athena.

- Athena â†’ Permite consultar e analisar os dados processados.

- VisualizaÃ§Ã£o â†’ GrÃ¡fico utilizando a biblioteca matplotlib, onde mostrei a quantidade de AÃ§Ãµes por Tipo. **(local no vs code)**

# Como rodar localmente (Vs Code)

1. Crie um ambiente virtual:
   
   python -m venv venv
   source venv/bin/activate  # Linux/macOS
   venv\Scripts\activate     # Windows
   

3. Instale as bibliotecas:
   
   pip install -r requirements.txt
   

4. Execute o scrao local:
   
   python lambda/scrap.py
   ```

ğŸ“‚ Estrutura do Projeto

ğŸ“¦ projeto-b3-aws
 â”£ ğŸ“‚ lambda
 â”ƒ â”— lambda_function.py   # CÃ³digo da funÃ§Ã£o Lambda
 â”£ ğŸ“‚ glue
 â”ƒ â”— glue.py   # CÃ³digo do script do glue
 â”ƒ  â”— glue.json   # CÃ³digo do script do glue
 â”£ ğŸ“‚ grÃ¡fico
 â”ƒ â”— grafico.py  # Script visualizaÃ§Ã£o dos dados
 â”£ ğŸ“‚ scrap_b3
 â”ƒ â”— ğŸ“œ scrap.py  # Script de coleta e upload para o S3
 â”£ ğŸ“œ README.md
 â”£ ğŸ“œ requirements.txt # arquivo com todas as bibliotecas para instalar.
 â”£ ğŸ“œ .gitignore
 
âš™ï¸ Tecnologias Utilizadas
'''
- [boto3](https://pypi.org/project/boto3/)
- [Pandas](https://pandas.pydata.org/)
- [lxml](https://lxml.de/)
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/)
- [pyarrow](https://pypi.org/project/pyarrow/)
- [Requests](https://docs.python-requests.org/)
- [selenium](https://pypi.org/project/selenium/)
- [webdriver-manager](https://pypi.org/project/webdriver-manager/)
- [PyAthena](https://pypi.org/project/PyAthena/)
- [matplotlib](https://pypi.org/project/matplotlib/)
- [seaborn](https://pypi.org/project/seaborn/)

âš™ï¸ ServiÃ§os AWS Utilizados
- Amazon S3 â†’ Armazenamento dos dados brutos e refinados.

- AWS Lambda â†’ FunÃ§Ã£o que aciona o Glue Job automaticamente.

- AWS Glue â†’ Job visual para ETL (extraÃ§Ã£o, transformaÃ§Ã£o e carga).

- AWS Glue Crawler â†’ Atualiza automaticamente o catÃ¡logo de dados.

- Amazon Athena â†’ Consulta SQL sobre os dados no S3.

ğŸ” PermissÃµes IAM
# Foram criadas 2 regras no IAM:

# Regra do Lambda com a polÃ­tica:

- AWSLambdaBasicExecutionRole

# Regra do Glue com as polÃ­ticas:

- AmazonS3FullAccess

- AWSGlueServiceRole

ğŸ“œ Lambda Function
# O cÃ³digo da funÃ§Ã£o Lambda nÃ£o estÃ¡ neste README, mas estÃ¡ localizado na pasta lambda/.

# FunÃ§Ã£o principal da Lambda:

- Ã‰ acionada automaticamente via evento S3 (PUT).

- Inicia o Glue Job responsÃ¡vel por processar os dados.

ğŸ”„ Fluxo do Pipeline
- O script scrap.py coleta os dados da B3 e envia para o bucket S3 na pasta scrap/date=YYYY-MM-DD/.

- O S3 Event Trigger detecta o upload e aciona a Lambda.

- A Lambda chama o Glue Job.

O Glue Job:

- Agrupa, renomeia e realiza cÃ¡lculo com datas.

- Salva os dados no S3 em refined/date=YYYY-MM-DD/acao=ABC/.

- O Glue Crawler atualiza o catÃ¡logo.

- O Athena lÃª os dados e permite consultas SQL.

ğŸ“Š Consultando no Athena
- Abra o Amazon Athena.

- No banco de dados configurado pelo Glue Crawler, selecione a tabela de dados refinados.

Execute consultas SQL, por exemplo:

SELECT * 
FROM dados_b3_2
WHERE date = '2025-08-05';

ğŸ‘¨â€ğŸ’» Autor

Gabriel Pereira Ferreira.